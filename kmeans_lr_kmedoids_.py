# -*- coding: utf-8 -*-
"""kmeans/lr/kmedoids/.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VEcgW-06zDF35R5HdHG4C-JfPvRQqHlH
"""

#Question 3
import numpy as np
from sklearn.datasets import load_digits
from sklearn.metrics import pairwise_distances_argmin

digits = load_digits()
X_digits = digits.data

def kmeans(X, n_clusters, max_iter=100):
    random_indices = np.random.permutation(X.shape[0])[:n_clusters]
    centers = X[random_indices]

    for _ in range(max_iter):
        labels = pairwise_distances_argmin(X, centers)

        new_centers = np.array([X[labels == i].mean(axis=0) for i in range(n_clusters)])

        if np.all(centers == new_centers):
            break
        centers = new_centers

    return centers, labels

def kmedoids(X, n_clusters, max_iter=100):
    indices = np.random.choice(X.shape[0], n_clusters, replace=False)
    medoids = X[indices]

    for _ in range(max_iter):
        distances = np.array([np.linalg.norm(X - medoid, axis=1) for medoid in medoids]).T

        labels = np.argmin(distances, axis=1)
        new_medoids = np.copy(medoids)
        for i in range(n_clusters):
            cluster_points = X[labels == i]
            if len(cluster_points) > 0:
                medoid_index = np.argmin([np.sum(np.linalg.norm(cluster_points - cluster_point, axis=1)) for cluster_point in cluster_points])
                new_medoids[i] = cluster_points[medoid_index]

        if np.array_equal(medoids, new_medoids):
            break
        medoids = new_medoids

    return medoids, labels

import matplotlib.pyplot as plt

centers_kmeans, labels_kmeans = kmeans(X_digits, n_clusters=10)

medoids_kmedoids, labels_kmedoids = kmedoids(X_digits, n_clusters=10)

def plot_digits(centroids):
    fig, ax = plt.subplots(2, 5, figsize=(8, 3))
    for axi, center in zip(ax.flat, centroids):
        axi.set(xticks=[], yticks=[])
        axi.imshow(center.reshape(8, 8), cmap='gray')
    plt.show()

print("k-means:")
plot_digits(centers_kmeans)
print("k-medoids:")
plot_digits(medoids_kmedoids)

class LogisticRegression:
    def __init__(self, learning_rate=0.01, reg_lambda=0.01, n_iterations=1000):
        self.learning_rate = learning_rate
        self.reg_lambda = reg_lambda
        self.n_iterations = n_iterations
        self.weights = None
        self.bias = None

    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))

    def fit(self, X, y):
        n_samples, n_features = X.shape
        self.weights = np.zeros(n_features)
        self.bias = 0

        for _ in range(self.n_iterations):
            model = np.dot(X, self.weights) + self.bias
            y_predicted = self.sigmoid(model)

            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y)) + (self.reg_lambda * self.weights)
            db = (1 / n_samples) * np.sum(y_predicted - y)

            self.weights -= self.learning_rate * dw
            self.bias -= self.learning_rate * db

    def predict(self, X):
        linear_model = np.dot(X, self.weights) + self.bias
        y_predicted = self.sigmoid(linear_model)
        return [1 if i > 0.5 else 0 for i in y_predicted]

#Question 2
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

data = load_breast_cancer()
X = data.data
y = data.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

log_reg = LogisticRegression(learning_rate=0.01, reg_lambda=0, n_iterations=1000)
log_reg.fit(X_train_scaled, y_train)

from sklearn.metrics import accuracy_score
y_pred = log_reg.predict(X_test_scaled)
print("Accuracy:", accuracy_score(y_test, y_pred))
#underfit?

from sklearn.preprocessing import PolynomialFeatures

poly = PolynomialFeatures(degree=2)
X_train_poly = poly.fit_transform(X_train_scaled)
X_test_poly = poly.transform(X_test_scaled)

log_reg.fit(X_train_poly, y_train)
y_pred_poly = log_reg.predict(X_test_poly)
print("Accuracy:", accuracy_score(y_test, y_pred_poly))
#Doesnt go past degree 2? Tried changing the degree but not enough ram :(

log_reg.reg_lambda = 0.1
log_reg.fit(X_train_poly, y_train)
y_pred_poly_reg = log_reg.predict(X_test_poly)
print("Accuracy:", accuracy_score(y_test, y_pred_poly_reg))
#Didnt change/reduce anything?
# -*- coding: utf-8 -*-
"""knn_DT_from_from_scratch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mS6ABIqcjCsO7tvgSn7VO4pgS8BK0TO2
"""

#Import scikit-learn dataset library
from sklearn import datasets
from math import sqrt
from collections import defaultdict
from collections import Counter
import numpy as np
#Load dataset
wine = datasets.load_wine()

from sklearn.model_selection import train_test_split

# Split dataset into training set and test set
X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.3) # 70% training and 30% test

#QUESTION 1 part c)
# I HAVE NO IDEA WHY but each time i run it the best accuracy switches between 2 and 3.
# I settled with 3 because I believe a larger K will decrease over fitting.
def distance(p1, p2):
  return np.sqrt(np.sum((p1 - p2) ** 2))

def labler(label_list):
  c0 = 0
  c1 = 0
  c2 = 0
  for label in label_list:
    if label == 1:
      c1 += 1
    if label == 0:
      c0 += 1
    if label == 2:
      c2 += 1
  if max(c0, c1, c2) == c0:
    return 0
  elif max(c0, c1, c2) == c1:
    return 1
  else:
    return 2

def find_neighbors(xtrain, ytrain, k):
  list_of_distances = []
  for i in range(len(xtrain)):
    distances = []
    for j in range(len(xtrain)):
        dist = distance(xtrain[i], xtrain[j])
        distances.append((ytrain[j], dist))
    distances.sort(key=lambda x: x[1])
    neighbors = distances[:k]
    list_of_distances.append(neighbors)
  return list_of_distances

def fit(xtrain, ytrain, k):
  neighbors = find_neighbors(xtrain,ytrain,k)

  labels = []
  final_list = []
  for sublist in neighbors:
    result_list = []
    for tup in sublist:
      result_list.append(tup[0])
    final_list.append(result_list)

  for i in range(len(final_list)):
    predicted_labels = labler(final_list[i])
    labels.append(predicted_labels)

  return labels


from sklearn.metrics import classification_report, accuracy_score

for i in range(1, 11):
  knnthingy = fit(X_train, y_train, k=i)
  print(f"k: {i}, Accuracy: {accuracy_score(y_train, knnthingy)}")

# print(fit(X_train, y_train, k=3))
# print(len(fit(X_train, y_train, k=3)))

#Part C) continued...
#So I am comparing my results to sklearns result that were shown in the colab link
#My accuracies are actually a bit higher than SKLEARNs. However my optimal k is much lower
# which makes me believe I may have overfitted a bit
knn_opt = fit(X_test, y_test, k=3)
#print(classification_report(y_test, knn_opt))
from sklearn import metrics
print("Train Accuracy:",metrics.accuracy_score(y_train, fit(X_train, y_train, k=3)))
print("Accuracy:",metrics.accuracy_score(y_test, knn_opt))

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
from sklearn.datasets import load_wine
from sklearn import tree
#convert to a dataframe
data = load_wine()
df = pd.DataFrame(data.data, columns = data.feature_names)
#create the species column
df['Class'] = data.target
#replace this with the actual names
target = np.unique(data.target)
target_names = np.unique(data.target_names)
targets = dict(zip(target, target_names))
df['Class'] = df['Class'].replace(targets)

#extract features and target variables
x = df.drop(columns="Class")
y = df["Class"]
#save the feature name and target variables
feature_names = x.columns
labels = y.unique()
#split the dataset
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(x,y,
                                                 test_size = 0.3,
                                                 random_state = 42)

#print(len(X_test))

#QUESTION 4
import numpy as np
# I used the min_sample_leaf = 10 and max_depth = 3 from the google colab link provided
# Comparing my results to SK learns it seems that my training accuracy is higher while
# my accuracy overall is lower than SK learns. This may mean that my tree overfits a little
# towards my training
class DecisionTree:
  def __init__(self, min_samples_leaf= 10):
    self.tree = {}
    self.splits = []
    self.min_samples_leaf = min_samples_leaf

  def entropy(self, data):
    classes, counts = np.unique(data[:, -1], return_counts=True)
    probabilities = counts / counts.sum()
    entropy = -np.sum(probabilities * np.log2(probabilities))
    return entropy

  def information_gain(self, data, feature_index, threshold):
    total_entropy = self.entropy(data)

    left_mask = data[:, feature_index] <= threshold
    right_mask = ~left_mask

    n_left, n_right = left_mask.sum(), right_mask.sum()
    if n_left == 0 or n_right == 0:
      return 0

    left_entropy = self.entropy(data[left_mask])
    right_entropy = self.entropy(data[right_mask])

    proportion_left = n_left / (n_left + n_right)
    proportion_right = n_right / (n_left + n_right)

    new_entropy = (proportion_left * left_entropy) + (proportion_right * right_entropy)
    information_gain = total_entropy - new_entropy

    return information_gain

  def find_best_split(self, data):
    best_feature_index = None
    best_threshold = None
    best_gain = -1

    for feature_index in range(data.shape[1] - 1):
      thresholds = np.unique(data[:, feature_index])
      for threshold in thresholds:
        gain = self.information_gain(data, feature_index, threshold)
        if gain > best_gain:
          best_gain = gain
          best_feature_index = feature_index
          best_threshold = threshold

    return best_feature_index, best_threshold

  def split(self, data, feature_index, threshold):
    left_mask = data[:, feature_index] <= threshold
    right_mask = ~left_mask
    left_data = data[left_mask]
    right_data = data[right_mask]
    return left_data, right_data

  def majority_class(self, labels):
    classes, counts = np.unique(labels, return_counts=True)
    majority_class = classes[np.argmax(counts)]
    return majority_class

  def build_tree(self, data, depth=0, max_depth=3):
    if (depth >= max_depth) or (len(np.unique(data[:, -1])) == 1) or (len(data) <= self.min_samples_leaf):
      return self.majority_class(data[:, -1])

    best_feature_index, best_threshold = self.find_best_split(data)
    if best_feature_index is None:
      return self.majority_class(data[:, -1])

    left_data, right_data = self.split(data, best_feature_index, best_threshold)

    split_rule = f"Feature {best_feature_index} <= {best_threshold}"
    self.splits.append((depth, split_rule))

    subtree = {}
    subtree['feature_index'] = best_feature_index
    subtree['threshold'] = best_threshold
    subtree['left'] = self.build_tree(left_data, depth + 1, max_depth)
    subtree['right'] = self.build_tree(right_data, depth + 1, max_depth)

    return subtree

  def fit(self, X, y, max_depth=3):
    data = np.column_stack((X, y))
    self.tree = self.build_tree(data, max_depth=max_depth)

  def predict_instance(self, instance, tree):
    if isinstance(tree, dict):
      feature_index = tree['feature_index']
      threshold = tree['threshold']
      if instance[feature_index] <= threshold:
        return self.predict_instance(instance, tree['left'])
      else:
        return self.predict_instance(instance, tree['right'])
    else:
      return tree

  def predict(self, X):
    X = np.array(X)
    return np.array([self.predict_instance(instance, self.tree) for instance in X])

  def print_splits(self):
    for depth, split_rule in self.splits:
      print(f"Depth {depth}: {split_rule}")


clf = DecisionTree()
clf.fit(X_train, y_train)
clf.predict(X_test)
#print(classification_report(y_test, clf.predict(X_test)))
clf.print_splits()
print("Train Accuracy:",metrics.accuracy_score(y_train, clf.predict(X_train)))
print("Accuracy:",metrics.accuracy_score(y_test, clf.predict(X_test)))
# print(len(clf.predict(X_test)))